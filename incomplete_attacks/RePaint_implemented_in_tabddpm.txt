
    @torch.no_grad()
    def reconstruct_RePaint(self, b, y_dist,
        known_features_mask,
        known_features_values,
        jumps=10,
        model_kwargs=None, cond_fn=None
    ):
        device = self.log_alpha.device
        z_norm_t = torch.randn((b, self.num_numerical_features), device=device)
        z_norm_t_minus_1 = z_norm_t

        for i in reversed(range(0, self.num_timesteps)):
            print(f"Sample timestep {i:4d}", end="\r")
            t = torch.full((b,), i, device=device, dtype=torch.long)

            x_num = known_features_values[:, : self.num_numerical_features]
            # x_cat = known_features_values[:, self.num_numerical_features:] # not needed here for my dataset



            ### (step 5, i.e. repeat steps 1 - 4 several times)
            for _ in range(jumps):

                ### step 1
                noise = torch.randn_like(x_num)
                x_t = self.gaussian_q_sample(x_num, t, noise=noise)

                ### step 2
                model_out = self._denoise_fn(z_norm_t, t, **out_dict)
                model_out_num = model_out[:, : self.num_numerical_features]
                # model_out_cat = model_out[:, self.num_numerical_features :] # again, no categoral features here, so ignore
                z_norm_t_minus_1 = self.gaussian_p_sample(
                    model_out_num,
                    z_norm_t,
                    t,
                    clip_denoised=False,
                    model_kwargs=model_kwargs,
                )["sample"]

                ### step 3
                z_norm_t_minus_1 = x_t * known_features_mask + z_norm_t_minus_1 * (1 - known_features_mask)

                ### step 4
                noise = torch.randn_like(x_num)
                # calling gaussian_q_sample() with (t+1) doesn't work for maximum t, but instead calling it with t should be fine
                z_norm_t = self.gaussian_q_sample(z_norm_t_minus_1, t, noise=noise)



        reconstruction = z_norm_t_minus_1.cpu()
        return reconstruction, out_dict