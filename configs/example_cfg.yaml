# W&B settings
wandb:
  project: "tabular-reconstruction-attacks"
  name: "PoPETs report"
  group: "<Attack_type>"
  tags:
    - "chained"
    - "development"

# Dataset settings
dataset:
  name: "nist_arizona_data"
  size: 10_000
  dir: "/home/golobs/data/reconstruction_data/adult/size_1k/sample_00"
QI: "QI1"

# Synthetic data generation
sdg_method: "MST"
sdg_params:
  eps: 10
  bins: 20

# Memorization comparison: run attack on both training and non-training targets
# When enabled, logs RA_train_*, RA_nontraining_*, and RA_delta_* (gap) metrics
# Point holdout_dir at a different disjoint sample (its train.csv is used as holdout)
memorization_test:
  enabled: false
  # holdout_dir: "/home/golobs/data/reconstruction_data/adult/size_1k/sample_01"

# Attack selection
attack_method: "RePaint"  # Name of the attack (see examples below)
data_type: "agnostic"     # "categorical", "continuous", or "agnostic" (for TabDDPM/RePaint)

# Attack parameters (specific to chosen attack)
attack_params:

  # ==========================================================================
  # ATTACK ENHANCEMENTS (optional)
  # These are composable wrappers that augment base attack methods
  # Each enhancement checks its own "enabled" flag and bypasses if disabled
  # ==========================================================================

  # Ensembling: Combines multiple attack methods for improved predictions
  ensembling:
    enabled: false  # Set to true to enable ensembling
    methods: ["RandomForest", "LightGBM", "KNN"]  # List of methods to ensemble
    aggregation: "voting"  # Options: "voting", "soft_voting", "averaging", "median", "weighted"
    include_primary: false  # Whether to include the primary attack_method in the ensemble
    # weights: [1.0, 1.5, 0.8]  # Optional: weights for weighted aggregation

  # Chaining: Predicts features sequentially, adding each to known features
  chaining:
    enabled: false  # Set to true to enable chaining
    order_strategy: "default"  # Options: "default", "manual", "correlation", "reverse_correlation", "mutual_info", "random"
    # order: ["F23", "F13", "F11", "F43"]  # Only used if order_strategy is "manual"
    log_intermediate: true  # Log per-feature accuracy during chaining
    random_seed: 42  # Seed for "random" order_strategy

  # TODO: Future enhancements
  # auxiliary_data:
  #   enabled: false
  #   source: "public_dataset"
  #   augmentation_ratio: 0.5

  # ==========================================================================
  # METHOD-SPECIFIC PARAMETERS
  # Parameters are automatically selected based on attack_method
  # ==========================================================================

  RePaint:
    num_epochs: 100_000
    resamples: 5
    jump: 10
    num_timesteps: 1000
    hidden_dims:
      - 512
      - 1024
      - 1024
      - 1024
      - 512
    dropout: 0.1
    jump_fn: "jump_max10"

  TabDDPM:
    retrain: true
    num_epochs: 100_000
    resamples: 10
    num_timesteps: 500
    hidden_dims:
      - 1024
      - 4096
      - 4096
      - 4096
      - 1024
    dropout: 0.0
    jump_fn: "jump_max10"
    batch_size: 2048
    lr: 0.0006

  RandomForest:
    max_depth: 25
    num_estimators: 25

  MLP:
    test_size: 0.2
    hidden_dims:
      - 128
      - 96
      - 64
    batch_size: 264
    learning_rate: 0.0003
    epochs: 250
    patience: 200
    dropout_rate: 0.2

  # Linear regression models
  LinearRegression:
    degree: 2
    max_iter: 100

  Ridge:
    alpha: 1.0
    max_iter: 100

  Lasso:
    alpha: 1.0
    max_iter: 100

  ElasticNet:
    alpha: 1.0
    l1_ratio: 0.5
    max_iter: 100

  SGDRegressor:
    alpha: 0.0001
    penalty: 'l2'
    max_iter: 1000

  KNN:
    k: 5
    use_weights: true

  LightGBM:
    num_estimators: 100
    objective: 'multiclass'
    metric: 'multi_logloss'
    verbosity: -1

  SVM:
    kernel: 'rbf'
    C: 1.0
    epsilon: 1.35

  Attention:
    num_heads: 4
    embedding_dim: 64
    num_layers: 2
    feedforward_dim: 128
    dropout_rate: 0.2
    test_size: 0.2
    batch_size: 128
    learning_rate: 0.001
    epochs: 100
    patience: 30

  AttentionAutoregressive:
    num_heads_AR: 4
    embedding_dim_AR: 64
    num_layers_AR: 2
    feedforward_dim_AR: 128
    dropout_rate_AR: 0.15
    test_size_AR: 0.2
    batch_size_AR: 64
    learning_rate_AR: 0.001
    epochs_AR: 200
    patience_AR: 30




# Other experiment settings
random_seed: 42


# =============================================================================
# ATTACK METHOD EXAMPLES
# =============================================================================
# Specify attack using attack_method + data_type (single source of truth)
#
# CATEGORICAL DATA (classification-based):
#   attack_method: "RandomForest"
#   data_type: "categorical"
#
#   attack_method: "LightGBM"
#   data_type: "categorical"
#
#   attack_method: "KNN"
#   data_type: "categorical"
#
#   attack_method: "SVM"
#   data_type: "categorical"
#
#   attack_method: "LogisticRegression"
#   data_type: "categorical"
#
#   attack_method: "NaiveBayes"
#   data_type: "categorical"
#
#   attack_method: "MLP"
#   data_type: "categorical"
#
#   attack_method: "Attention"
#   data_type: "categorical"
#
#   attack_method: "Mode"  # baseline
#   data_type: "categorical"
#
# CONTINUOUS DATA (regression-based):
#   attack_method: "RandomForest"
#   data_type: "continuous"
#
#   attack_method: "LightGBM"
#   data_type: "continuous"
#
#   attack_method: "KNN"
#   data_type: "continuous"
#
#   attack_method: "SVM"
#   data_type: "continuous"
#
#   attack_method: "LinearRegression"
#   data_type: "continuous"
#
#   attack_method: "Ridge"
#   data_type: "continuous"
#
#   attack_method: "Lasso"
#   data_type: "continuous"
#
#   attack_method: "MLP"
#   data_type: "continuous"
#
#   attack_method: "Mean"  # baseline
#   data_type: "continuous"
#
#   attack_method: "Median"  # baseline
#   data_type: "continuous"
#
# DATA-TYPE AGNOSTIC (work on both categorical and continuous):
#   attack_method: "TabDDPM"
#   data_type: "agnostic"  # or omit data_type
#
#   attack_method: "RePaint"
#   data_type: "agnostic"  # or omit data_type


# =============================================================================
# CHAINING EXAMPLES
# =============================================================================
# Below are example chaining configurations for different use cases:
#
# Example 1: No chaining (default behavior)
# chaining:
#   enabled: false
#
# Example 2: Chaining with default order (order in hidden_features list)
# chaining:
#   enabled: true
#   order_strategy: "default"
#   log_intermediate: true
#
# Example 3: Manual ordering (specify exact prediction order)
# chaining:
#   enabled: true
#   order_strategy: "manual"
#   order: ["F23", "F13", "F11", "F43", "F36", "F15", "F33", "F25"]
#   log_intermediate: true
#
# Example 4: Correlation-based ordering (predict most correlated with QI first)
# chaining:
#   enabled: true
#   order_strategy: "correlation"
#   log_intermediate: true
#
# Example 5: Reverse correlation (predict least correlated with QI first)
# chaining:
#   enabled: true
#   order_strategy: "reverse_correlation"
#   log_intermediate: true
#
# Example 6: Mutual information ordering
# chaining:
#   enabled: true
#   order_strategy: "mutual_info"
#   log_intermediate: true
#
# Example 7: Random ordering (for baseline comparison)
# chaining:
#   enabled: true
#   order_strategy: "random"
#   random_seed: 42
#   log_intermediate: true


# =============================================================================
# ENSEMBLING EXAMPLES
# =============================================================================
# Below are example ensembling configurations for different use cases:
#
# Example 1: No ensembling (default behavior - single attack method)
# ensembling:
#   enabled: false
#
# Example 2: Simple majority voting ensemble (categorical data)
# ensembling:
#   enabled: true
#   methods: ["RandomForest", "LightGBM", "KNN"]
#   aggregation: "voting"
#   include_primary: false  # methods list is exhaustive
#
# Example 3: Ensemble with primary method included (4 models total)
# ensembling:
#   enabled: true
#   methods: ["LightGBM", "KNN", "SVM"]
#   aggregation: "voting"
#   include_primary: true  # primary attack_method is added to the ensemble
#
# Example 4: Weighted ensemble (give more weight to better models)
# ensembling:
#   enabled: true
#   methods: ["RandomForest", "LightGBM", "MLP"]
#   aggregation: "weighted"
#   include_primary: false
#   weights: [1.5, 2.0, 1.0]  # LightGBM gets 2x weight
#
# Example 5: Averaging ensemble (for continuous data)
# ensembling:
#   enabled: true
#   methods: ["RandomForest", "Ridge", "KNN"]
#   aggregation: "averaging"
#   include_primary: false
#
# Example 6: Median ensemble (robust to outliers, continuous data)
# ensembling:
#   enabled: true
#   methods: ["LinearRegression", "Ridge", "Lasso"]
#   aggregation: "median"
#   include_primary: true
#
# Example 7: Soft voting (probability-based, if available)
# ensembling:
#   enabled: true
#   methods: ["RandomForest", "LightGBM", "MLP"]
#   aggregation: "soft_voting"
#   include_primary: false


# =============================================================================
# COMBINING ENHANCEMENTS: ENSEMBLING + CHAINING
# =============================================================================
# Enhancements are composable! You can use both ensembling and chaining together.
# Order of application: Ensembling first (combines methods), then Chaining (sequential prediction)
#
# Example: Ensemble of 3 methods with mutual-info chaining
# ensembling:
#   enabled: true
#   methods: ["RandomForest", "LightGBM"]
#   aggregation: "voting"
#   include_primary: true  # Total of 3 models if primary is different
#
# chaining:
#   enabled: true
#   order_strategy: "mutual_info"
#   log_intermediate: true
